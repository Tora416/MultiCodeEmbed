# MultiCodeEmbed
*Multi-Dimension Code Embedding*——
A three-dimensional degree code embedding tool that generates vectors in contextual, syntactic, and semantic dimensions, respectively. Based on [code2vec](https://github.com/dcoimbra/dx2021), [CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection) and [SBERT](https://github.com/UKPLab/sentence-transformers).

This project was created to help **[Forsee](https://github.com/keepTheFlowerOfTime/Forsee)** process data.

## 📄 Dataset
This project uses the dataset from the ***Forsee*** project, [Link to dataset](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

## 🗂️ Output Data

Some of the output files (incluing results and models) were so large that I put them in cloud drives, here is a list of those files.

These files are split into two parts and uploaded to [Aliyun Drive](https://www.alipan.com/s/X86BbiFoRR5) and [Google Drive](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing). You can download them by clicking on the links.

- **Part1**: CodeBERT fine-tuned weights (binary). `./CodeBERT/code/saved_models/checkpoint-best-acc/{language}_model.bin`. Please download from [![Static Badge](https://img.shields.io/badge/Google-red)](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing).
- **Part2**：Other files. Please download from [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5).

⚠ **IMPORTANT NOTE**: *In order to bypass the platform restrictions, the files uploaded on [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5) have `.txt` extensions added to them, so please remove these `.txt` extensions before using them.*

### Overview

```
MultiCodeEmbed/
|
├─ ...
├─ code2vec/
|  └─ dx2021/
|       ├─ ...
|       └─ code2vec/
|            ├─ ...
|            ├─ models/
|            |    ├─ ...
|            |    └─ {language}/
|            |         ├─ checkpoint
|            |         ├─ dictionaries.bin
|            |         ├─ saved_model.release.data-00000-of-00001
|            |         ├─ saved_model.release.index
|            |         └─ saved_model.release.meta
|            └─ output/
|                 ├─ c_code2vec_embeddings.npy
|                 ├─ cpp_code2vec_embeddings.npy
|                 └─ java_code2vec_embeddings.npy
|
├─ CodeBERT/
|  ├─ ...
|  ├─ code/
|  |    └─ saved_models/
|  |         └─ checkpoint-best-acc/
|  |              ├─ c_model.bin
|  |              ├─ cpp_model.bin
|  |              └─ java_model.bin
|  └─ dataset/
|       ├─ c_CodeBERT_base_embeddings.npy
|       ├─ c_CodeBERT_embeddings.npy
|       ├─ cpp_CodeBERT_base_embeddings.npy
|       ├─ cpp_CodeBERT_embeddings.npy
|       ├─ java_CodeBERT_base_embeddings.npy
|       └─ java_CodeBERT_embeddings.npy
|
└─ SBERT/
   └─ output/
        ├─ c_SBERT_embeddings.npy
        ├─ cpp_SBERT_embeddings.npy
        └─ java_SBERT_embeddings.npy
```

### Results
The results are all `*.npy` files.

The project produced embeddings generated by the code2vec, SBERT, and CodeBERT models, each of which produced three files corresponding to the datasets in each of the three languages.

- **SBERT**:`{language}_SBERT_embeddings.npy`. *DirPath*:`./SBERT/output/`.
- **CodeBERT** (using the pre-trained model directly):`{language}_CodeBERT_base_embeddings.npy`. *DirPath*:`./CodeBERT/dataset/`.
- **CodeBERT** (using the fine-tuned model):`{language}_CodeBERT_embeddings.npy`. *DirPath*:`./CodeBERT/dataset/`.
- **code2vec**:`{language}_code2vec_embeddings.npy`. *DirPath*:`./code2vec/dx2021/code2vec/output/`.

### Trained models

This project provides trained code2vec models. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **code2vec**: `{language}/saved_model.release.*`, `{language}/dictionaries.bin` and `{language}/checkpoint`. They needs to be placed under the `./code2vec/dx2021/code2vec/models/`

### Fine-tuned weights (binary)

The project also provides trained fine-tuned weights for use in the CodeBERT model. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **CodeBERT**：`{language}_model.bin`.It needs to be placed under the `./CodeBERT/code/saved_models/checkpoint-best-acc/`.

## 🖥️ Steps to Run

### Preprocess

Please create a venv virtual environment for Python in the root directory.

(⚠ **We strongly recommend that you do this step, as the code in the project looks for Python paths based on this premise. If you do not want to do this step, please modify the Python path in the code.**)

Then please use the command below to install the dependent libraries.

```powershell
your_python_path -m pip install -r requirements.txt
```

The dataset needs to be converted from catalog to `jsonl` format to be processed.
Please run the command below in the project root directory to process the dataset.

```powershell
your_python_path ./Code/Preprocess.py
```

### Start

Please run the command below in the project root directory to accomplish the functions you need.

```powershell
your_python_path ./Code/runMCE.py <args>
```

### Args

#### Train code2vec

- If you want to train the code2vec model, use argument `-Tc` or `--train_code2vec`.
- ***If you downloaded and placed the release model, you can skip this step.***

#### Run code2vec

- Use argument `-Rc` or `--run_code2vec` if you want the embedded result of code2vec.

#### Train CodeBERT

- If you want to train the CodeBERT model, use argument `-TC` or `--train_codebert`.
- ***If you downloaded and placed the model, you can skip this step.***
- ***You can also skip this step if you don't want to use a fine-tuned model.***

#### Run CodeBERT

- If you want to get the embedding result of CodeBERT, use the parameters `-RC` and `--run_codebert`. Add parameters `-B` and `--run_codebert_base` if you don't want to use a fine-tuned model. Such as `-RC -B` or `--run_codebert --run_codebert_base`.
- Using `-B` or `--run_codebert_base` alone has no effect.
- ⚠ **`--train_codebert`(or `-TC`) and `--run_codebert_base`(or `-B`) cannot be used at the same time.**

#### Run SBERT

- If you want to get the embedding result of SBERT, use the parameters `-RS` and `--run_sbert`. 

## 🧩 Built Upon Giants
This project leverages cutting-edge research from the open-source community:
- **[VulSim](https://github.com/SamihaShimmi/VulSim)**
- **[code2vec (Used in VulSim)](https://github.com/dcoimbra/dx2021)**
- **[code2vec (Vanilla)](https://github.com/tech-srl/code2vec)**
- **[astminer](https://github.com/JetBrains-Research/astminer)**  
- **[CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection)**  
- **[SBERT](https://github.com/UKPLab/sentence-transformers)**  
