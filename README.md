# MultiCodeEmbed
*Multi-Dimension Code Embedding*——
A three-dimensional degree code embedding tool that generates vectors in contextual, syntactic, and semantic dimensions, respectively. Based on [code2vec](https://github.com/dcoimbra/dx2021), [CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection) and [SBERT](https://github.com/UKPLab/sentence-transformers).

This project was created to help **[Forsee](https://github.com/keepTheFlowerOfTime/Forsee)** process data.

## 📄 Dataset
This project uses the dataset from the ***Forsee*** project, [Link to dataset](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

## 🚧 Current Status 

- [x] Dataset Processing ![Static Badge](https://img.shields.io/badge/Done-green)
- [x] Model Adaptation
  - [x] SBERT ![Static Badge](https://img.shields.io/badge/Done-green)
  - [x] CodeBERT ![Static Badge](https://img.shields.io/badge/Done-green)
  - [x] code2vec ![Static Badge](https://img.shields.io/badge/Done-green)
- [ ] Documentation ![Static Badge](https://img.shields.io/badge/WIP-orange)

## 🗂️ Output Data

Some of the output files were so large that I put them in release, here is a list of those files.

These files are split into two parts and uploaded to [Aliyun Drive](https://www.alipan.com/s/X86BbiFoRR5) and [Google Drive](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing). You can download them by clicking on the links.

- **Part1**: CodeBERT fine-tuned weights (binary). `./CodeBERT/code/saved_models/checkpoint-best-acc/{language}_model.bin`. Please download from [![Static Badge](https://img.shields.io/badge/Google-red)](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing).
- **Part2**：Other files. Please download from [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5).

⚠ **IMPORTANT NOTE**: *In order to bypass the platform restrictions, the files uploaded on [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5) have `.txt` extensions added to them, so please remove these `.txt` extensions before using them.*

### Overview

```
MultiCodeEmbed/
|
├─ ...
├─ code2vec/
|  └─ dx2021/
|       ├─ ...
|       └─ code2vec/
|            ├─ ...
|            ├─ models/
|            |    ├─ ...
|            |    └─ {language}/
|            |         ├─ checkpoint
|            |         ├─ dictionaries.bin
|            |         ├─ saved_model.release.data-00000-of-00001
|            |         ├─ saved_model.release.index
|            |         └─ saved_model.release.meta
|            └─ output/
|                 ├─ c_code2vec_embeddings.npy
|                 ├─ cpp_code2vec_embeddings.npy
|                 └─ java_code2vec_embeddings.npy
|
├─ CodeBERT/
|  ├─ ...
|  ├─ code/
|  |    └─ saved_models/
|  |         └─ checkpoint-best-acc/
|  |              ├─ c_model.bin
|  |              ├─ cpp_model.bin
|  |              └─ java_model.bin
|  └─ dataset/
|       ├─ c_CodeBERT_base_embeddings.npy
|       ├─ c_CodeBERT_embeddings.npy
|       ├─ cpp_CodeBERT_base_embeddings.npy
|       ├─ cpp_CodeBERT_embeddings.npy
|       ├─ java_CodeBERT_base_embeddings.npy
|       └─ java_CodeBERT_embeddings.npy
|
└─ SBERT/
   └─ output/
        ├─ c_SBERT_embeddings.npy
        ├─ cpp_SBERT_embeddings.npy
        └─ java_SBERT_embeddings.npy
```

### Results
The results are all `*.npy` files.

The project produced embeddings generated by the code2vec, SBERT, and CodeBERT models, each of which produced three files corresponding to the datasets in each of the three languages.

- **SBERT**:`{language}_SBERT_embeddings.npy`
- **CodeBERT** (using the pre-trained model directly):`{language}_CodeBERT_base_embeddings.npy`
- **CodeBERT** (using the fine-tuned model):`{language}_CodeBERT_embeddings.npy`
- **code2vec**:`{language}_code2vec_embeddings.npy`

### Trained models

This project provides trained code2vec models. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **code2vec**: `{language}/saved_model.release.*`, `{language}/dictionaries.bin` and `{language}/checkpoint`. They needs to be placed under the `./code2vec/dx2021/code2vec/models/`

### Fine-tuned weights (binary)

The project also provides trained fine-tuned weights for use in the CodeBERT model. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **CodeBERT**：`{language}_model.bin`.It needs to be placed under the `./CodeBERT/code/saved_models/checkpoint-best-acc/`.

## 🖥️ Steps to run

### Step1

pass

### Step2

pass

### StepN

## 🧩 Built Upon Giants
This project leverages cutting-edge research from the open-source community:
- **[VulSim](https://github.com/SamihaShimmi/VulSim)**
- **[code2vec](https://github.com/dcoimbra/dx2021)**  
- **[CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection)**  
- **[SBERT](https://github.com/UKPLab/sentence-transformers)**  
