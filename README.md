# MultiCodeEmbed
*Multi-Dimension Code Embedding*â€”â€”
A three-dimensional degree code embedding tool that generates vectors in contextual, syntactic, and semantic dimensions, respectively. Based on [code2vec](https://github.com/dcoimbra/dx2021), [CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection) and [SBERT](https://github.com/UKPLab/sentence-transformers).

This project was created to help **[Forsee](https://github.com/keepTheFlowerOfTime/Forsee)** process data.

## ğŸ“„ Dataset
This project uses the dataset from the ***Forsee*** project, [Link to dataset](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

## ğŸš§ Current Status 

- [x] Dataset Processing ![Static Badge](https://img.shields.io/badge/Done-green)
- [x] Model Adaptation
  - [x] SBERT ![Static Badge](https://img.shields.io/badge/Done-green)
  - [x] CodeBERT ![Static Badge](https://img.shields.io/badge/Done-green)
  - [x] code2vec ![Static Badge](https://img.shields.io/badge/Done-green)
- [ ] Documentation ![Static Badge](https://img.shields.io/badge/WIP-orange)

## ğŸ—‚ï¸ Output Data

Some of the output files were so large that I put them in release, here is a list of those files.

These files are split into two parts and uploaded to [Aliyun Drive](https://www.alipan.com/s/X86BbiFoRR5) and [Google Drive](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing). You can download them by clicking on the links.

- **Part1**: CodeBERT fine-tuned weights (binary). `./CodeBERT/code/saved_models/checkpoint-best-acc/{language}_model.bin`. Please download from [![Static Badge](https://img.shields.io/badge/Google-red)](https://drive.google.com/drive/folders/1bfgcuEFpK-3Pg1uoW9e7B9iQGFjJuFFC?usp=sharing).
- **Part2**ï¼šOther files. Please download from [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5).

âš  **IMPORTANT NOTE**: *In order to bypass the platform restrictions, the files uploaded on [![Static Badge](https://img.shields.io/badge/Aliyun-blue)](https://www.alipan.com/s/X86BbiFoRR5) have `.txt` extensions added to them, so please remove these `.txt` extensions before using them.*

### Overview

```
MultiCodeEmbed/
|
â”œâ”€ ...
â”œâ”€ code2vec/
|  â””â”€ dx2021/
|       â”œâ”€ ...
|       â””â”€ code2vec/
|            â”œâ”€ ...
|            â”œâ”€ models/
|            |    â”œâ”€ ...
|            |    â””â”€ {language}/
|            |         â”œâ”€ checkpoint
|            |         â”œâ”€ dictionaries.bin
|            |         â”œâ”€ saved_model.release.data-00000-of-00001
|            |         â”œâ”€ saved_model.release.index
|            |         â””â”€ saved_model.release.meta
|            â””â”€ output/
|                 â”œâ”€ c_code2vec_embeddings.npy
|                 â”œâ”€ cpp_code2vec_embeddings.npy
|                 â””â”€ java_code2vec_embeddings.npy
|
â”œâ”€ CodeBERT/
|  â”œâ”€ ...
|  â”œâ”€ code/
|  |    â””â”€ saved_models/
|  |         â””â”€ checkpoint-best-acc/
|  |              â”œâ”€ c_model.bin
|  |              â”œâ”€ cpp_model.bin
|  |              â””â”€ java_model.bin
|  â””â”€ dataset/
|       â”œâ”€ c_CodeBERT_base_embeddings.npy
|       â”œâ”€ c_CodeBERT_embeddings.npy
|       â”œâ”€ cpp_CodeBERT_base_embeddings.npy
|       â”œâ”€ cpp_CodeBERT_embeddings.npy
|       â”œâ”€ java_CodeBERT_base_embeddings.npy
|       â””â”€ java_CodeBERT_embeddings.npy
|
â””â”€ SBERT/
   â””â”€ output/
        â”œâ”€ c_SBERT_embeddings.npy
        â”œâ”€ cpp_SBERT_embeddings.npy
        â””â”€ java_SBERT_embeddings.npy
```

### Results
The results are all `*.npy` files.

The project produced embeddings generated by the code2vec, SBERT, and CodeBERT models, each of which produced three files corresponding to the datasets in each of the three languages.

- **SBERT**:`{language}_SBERT_embeddings.npy`
- **CodeBERT** (using the pre-trained model directly):`{language}_CodeBERT_base_embeddings.npy`
- **CodeBERT** (using the fine-tuned model):`{language}_CodeBERT_embeddings.npy`
- **code2vec**:`{language}_code2vec_embeddings.npy`

### Trained models

This project provides trained code2vec models. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **code2vec**: `{language}/saved_model.release.*`, `{language}/dictionaries.bin` and `{language}/checkpoint`. They needs to be placed under the `./code2vec/dx2021/code2vec/models/`

### Fine-tuned weights (binary)

The project also provides trained fine-tuned weights for use in the CodeBERT model. Training on the dataset used by [Forsee](https://github.com/keepTheFlowerOfTime/Forsee/tree/main/dataset).

- **CodeBERT**ï¼š`{language}_model.bin`.It needs to be placed under the `./CodeBERT/code/saved_models/checkpoint-best-acc/`.

## ğŸ–¥ï¸ Steps to run

### Step1

pass

### Step2

pass

### StepN

## ğŸ§© Built Upon Giants
This project leverages cutting-edge research from the open-source community:
- **[VulSim](https://github.com/SamihaShimmi/VulSim)**
- **[code2vec](https://github.com/dcoimbra/dx2021)**  
- **[CodeBERT](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/Defect-detection)**  
- **[SBERT](https://github.com/UKPLab/sentence-transformers)**  
